# Using Large Language Models for Retrieval-Augmented Generation in Actuarial Science

## Overview
This repository features all the materials from the paper, which focused on enhancing the performance of Large Language Models (LLMs) within the actuarial science field using advanced machine learning techniques. The project specifically explores the post-training effects on the Mistral 7B model utilizing a Swiss regulatory document concerning tied assets for insurance companies. Through the application of fine-tuning and Retrieval-Augmented Generation (RAG) methods, the study aimed to improve the model's ability to process and interpret complex document content. Although fine-tuning showed minimal improvement in the model's response precision, the incorporation of RAG significantly increased the accuracy and relevance of the responses. This outcome illustrates that LLMs, when augmented with RAG, can effectively comprehend and contextualize beyond their initial training sets, thereby offering more precise and timely solutions to actuarial queries.

## Contents
- `Answers of GPT4 Model.pdf`: Chat with the GPT-4 model showcasing answers generated by the model.
- `Score by GPT4 for Fine-Tuned Model with RAG and Base Model with RAG.pdf`: Comparative scores generated by the GPT-4 model for Fine-Tuned Mitsral 7B model with Retrieval-Augmented Generation (RAG) and the base Mistral 7B model with RAG.
- `Score by GPT4 for Fine-tuned Model and GPT4.pdf`: Scores generated by the GPT-4 model comparing the fine-tuned Mistral 7B model against the standard GPT-4 model.
- `Score by GPT4 for Fine-tuned Model with RAG and GPT4.pdf`: Scores generated by the GPT-4 model assessing the performance of the fine-tuned Mistral 7B model with RAG compared to the standard GPT-4.
- `Manual Scoring.xlsx`: Manual evaluation between the models.
- `finma rs 2016 05.pdf` and `finma rs 2016 05 en.pdf`: The original regulatory document and its English translation used for creating the training dataset.
- `LLM_for_Actuarial_Science.ipynb`: Jupyter notebook containing the code for fine-tuning the Mistral 7B model, implementing RAG, and evaluating the models.

## Usage
To replicate the fine-tuning and testing process, follow these steps:
1. Ensure you have a Python environment with Jupyter installed, or prefer to use Google Colab for an environment that requires no setup.
2. Download and open the notebook.
3. If you are running in Google Colab, before running the notebook, ensure to switch to a GPU runtime:
   - From the menu in Colab, go to `Runtime` > `Change runtime type`.
   - Choose `GPU` as the hardware accelerator.
4. Follow the instructions in the notebook to install any required libraries and run the cells.

